{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "big-data-cve.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "WljrYMuLC_oz",
        "colab_type": "code",
        "outputId": "3ebbb8ac-59a9-486b-cbde-2cd2ec705476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get -y install openjdk-8-jdk-headless\n",
        "!wget http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install pyspark_dist_explore findspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk-headless is already the newest version (8u191-b12-2ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "--2019-03-06 02:08:28--  http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
            "Resolving apache.osuosl.org (apache.osuosl.org)... 140.211.166.134, 64.50.233.100, 64.50.236.52, ...\n",
            "Connecting to apache.osuosl.org (apache.osuosl.org)|140.211.166.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 227893062 (217M) [application/x-gzip]\n",
            "Saving to: ‘spark-2.4.0-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.4.0-bin-had 100%[===================>] 217.33M  71.3MB/s    in 3.0s    \n",
            "\n",
            "2019-03-06 02:08:31 (71.3 MB/s) - ‘spark-2.4.0-bin-hadoop2.7.tgz’ saved [227893062/227893062]\n",
            "\n",
            "Collecting pyspark_dist_explore\n",
            "  Downloading https://files.pythonhosted.org/packages/70/36/1c6c12d65a80cdc2e5c645e7bac39e2d37243e14e60dafee1e1b442c6063/pyspark_dist_explore-0.1.5-py3-none-any.whl\n",
            "Collecting findspark\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/c8/e6e1f6a303ae5122dc28d131b5a67c5eb87cbf8f7ac5b9f87764ea1b1e1e/findspark-1.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (1.14.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (0.22.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyspark_dist_explore) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (2.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pyspark_dist_explore) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pyspark_dist_explore) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pyspark_dist_explore) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyspark_dist_explore) (40.8.0)\n",
            "Installing collected packages: pyspark-dist-explore, findspark\n",
            "Successfully installed findspark-1.3.0 pyspark-dist-explore-0.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LSbvlZ9jDVM7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# setup environment variables\n",
        "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-2.4.0-bin-hadoop2.7'\n",
        "os.environ['KAGGLE_USERNAME'] = 'ronaldsumbayak'\n",
        "os.environ['KAGGLE_KEY'] = '771009dffffa0cd0883d5fb98594d756'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d10LJSgrGRYv",
        "colab_type": "code",
        "outputId": "87f8f08e-4288-489b-d112-8e85531bd6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "!kaggle datasets download -d kmader/aminer-academic-citation-dataset\n",
        "!unzip aminer-academic-citation-dataset.zip -d data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading aminer-academic-citation-dataset.zip to /content\n",
            "100% 2.27G/2.27G [00:41<00:00, 15.5MB/s]\n",
            "100% 2.27G/2.27G [00:41<00:00, 58.4MB/s]\n",
            "Archive:  aminer-academic-citation-dataset.zip\n",
            "  inflating: data/dblp-ref-1.json    \n",
            "  inflating: data/citation-network2.txt  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxmhZkNiDhtd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNmya1-bEN46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark_dist_explore import hist\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QpMKw2dIbXfw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZBLFg3HsF4CK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "dblp_refs = [spark.read.json('data/dblp-ref-0.json'),\n",
        "             spark.read.json('data/dblp-ref-1.json'),\n",
        "             spark.read.json('data/dblp-ref-2.json'),\n",
        "             spark.read.json('data/dblp-ref-3.json')]\n",
        "df = reduce(lambda x, f: f.union(x), dblp_refs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Tx4fOI0VS0Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Ejk-k4TVRmf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KH-6jBmKIv36",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "annual_paper_count = df.groupBy('year').count().orderBy('year', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y_67vZj2mF38",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "annual_paper_count.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2SflY5lYWJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "annual_paper_count.toPandas().plot(x='year', y='count')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HzWfM2y-d9w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_citation_avg = df.agg(avg('n_citation')).collect()[0][0]\n",
        "print('Average number of citation per paper:', int(n_citation_avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8qCAI6GFqAZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "yv = df.groupBy('year', 'venue') \\\n",
        "       .agg(count('*').alias('count')) \\\n",
        "       .where('venue != \"\"')\n",
        "ym = df.groupBy('year', 'venue') \\\n",
        "       .agg(count('*').alias('count')) \\\n",
        "       .where('venue != \"\"') \\\n",
        "       .groupBy('year') \\\n",
        "       .agg(max('count').alias('count'))\n",
        "\n",
        "vy = ym.join(yv, ['year', 'count'], 'left') \\\n",
        "       .orderBy('year', ascending=False)\n",
        "vy.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}